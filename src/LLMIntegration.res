// LLM integration layer with guardrails
// CRITICAL: Never auto-commit LLM output; always requiresApproval: true

open Types

// Anthropic API binding (simplified)
type anthropicClient = {apiKey: string}

type message = {
  role: string,
  content: string,
}

type anthropicRequest = {
  model: string,
  messages: array<message>,
  max_tokens: int,
  temperature: float,
}

@module("@anthropic-ai/sdk") @new
external createAnthropicClient: {..} => anthropicClient = "Anthropic"

// LLM prompt templates
let generateSecurityMdPrompt = (repoContext: string): string => {
  `You are tasked with generating a SECURITY.md file for a software project.

Repository context:
${repoContext}

Generate a comprehensive SECURITY.md file that includes:
1. Supported versions
2. How to report a vulnerability
3. Security update policy
4. Contact information

Output only the markdown content, no additional commentary.`
}

let generateContributingPrompt = (repoContext: string): string => {
  `You are tasked with generating a CONTRIBUTING.md file for a software project.

Repository context:
${repoContext}

Generate a comprehensive CONTRIBUTING.md file that includes:
1. How to contribute
2. Development setup
3. Code style guidelines
4. Pull request process
5. Code of conduct reference

Output only the markdown content, no additional commentary.`
}

let suggestConflictResolutionPrompt = (
  conflict: conflict,
  documents: array<document>,
): string => {
  let docContents =
    documents
    ->Belt.Array.mapWithIndex((idx, doc) => {
      `Document ${idx->Int.toString + 1} (${doc.metadata.path}):
---
${doc.content}
---`
    })
    ->Js.Array2.joinWith("\n\n")

  `You are tasked with suggesting a resolution for a documentation conflict.

Conflict type: ${switch conflict.conflictType {
    | DuplicateContent => "Duplicate content"
    | VersionMismatch => "Version mismatch"
    | CanonicalConflict => "Canonical source conflict"
    | StructuralConflict => "Structural conflict"
    | SemanticConflict => "Semantic conflict"
    }}

Conflicting documents:
${docContents}

Analyze these documents and suggest which one should be kept, or if they should be merged.
Provide your reasoning and confidence level (0.0-1.0).

Response format:
{
  "selected_document_index": <index or "merge">,
  "confidence": <0.0-1.0>,
  "reasoning": "<your analysis>"
}`
}

// Call LLM API with guardrails
let callLLM = async (
  provider: llmProvider,
  promptType: llmPromptType,
  context: string,
): result<llmResponse, string> => {
  try {
    // Build prompt based on type
    let prompt = switch promptType {
    | GenerateSecurityMd => generateSecurityMdPrompt(context)
    | GenerateContributing => generateContributingPrompt(context)
    | GenerateSupport => `Generate a SUPPORT.md file with support resources and contact info for: ${context}`
    | SuggestConflictResolution => context // Already formatted
    | ImproveDocumentation => `Improve the following documentation:\n\n${context}`
    }

    switch provider {
    | Anthropic(apiKey) => {
        // Call Anthropic API
        // Note: This is a simplified version - real implementation would use proper SDK
        let response = {
          content: `[LLM GENERATED CONTENT - REQUIRES APPROVAL]\n\n${prompt}`,
          confidence: 0.7,
          requiresApproval: true, // ALWAYS true for LLM output
          reasoning: "Generated by LLM - requires human review before use",
          model: "claude-3-sonnet-20240229",
        }

        Ok(response)
      }
    | OpenAI(apiKey) => {
        // Similar implementation for OpenAI
        Error("OpenAI integration not yet implemented")
      }
    | Local(modelPath) => {
        // Local model integration
        Error("Local model integration not yet implemented")
      }
    }
  } catch {
  | exn =>
    Error(
      `LLM API call failed: ${exn->Js.Exn.message->Belt.Option.getWithDefault("Unknown error")}`,
    )
  }
}

// Generate missing documentation using LLM
let generateMissingDoc = async (
  documentType: documentType,
  repoContext: string,
  provider: llmProvider,
): result<llmResponse, string> => {
  let promptType = switch documentType {
  | SECURITY => GenerateSecurityMd
  | CONTRIBUTING => GenerateContributing
  | SUPPORT => GenerateSupport
  | _ => ImproveDocumentation
  }

  await callLLM(provider, promptType, repoContext)
}

// LLM-assisted conflict resolution
let suggestResolution = async (
  conflict: conflict,
  provider: llmProvider,
): result<llmResponse, string> => {
  let prompt = suggestConflictResolutionPrompt(conflict, conflict.documents)
  await callLLM(provider, SuggestConflictResolution, prompt)
}

// Validate LLM output (basic checks)
let validateLLMOutput = (response: llmResponse, docType: documentType): result<unit, string> => {
  // Check content is not empty
  if Js.String2.trim(response.content) == "" {
    return Error("LLM generated empty content")
  }

  // Check for common issues
  if Js.String2.includes(response.content, "[TODO]") {
    return Error("LLM output contains unresolved TODOs")
  }

  // Document-specific validation
  switch docType {
  | SECURITY => {
      if !Js.String2.includes(response.content, "Security") &&
        !Js.String2.includes(response.content, "vulnerability") {
        Error("SECURITY.md should mention security or vulnerabilities")
      } else {
        Ok()
      }
    }
  | LICENSE => Error("Should never generate LICENSE files with LLM")
  | _ => Ok()
  }
}

// Create document from LLM response with guardrails
let createDocumentFromLLM = (
  response: llmResponse,
  documentType: documentType,
  repository: string,
): result<document, string> => {
  // CRITICAL: Validate output
  switch validateLLMOutput(response, documentType) {
  | Error(msg) => Error(msg)
  | Ok() => {
      // Create metadata with explicit warning
      let metadata: documentMetadata = {
        path: `[LLM_GENERATED]/${documentTypeToString(documentType)}.md`,
        documentType: documentType,
        lastModified: Js.Date.now(),
        version: None,
        canonicalSource: Inferred,
        repository: repository,
        branch: "llm-generated",
      }

      let doc = Deduplicator.createDocument(response.content, metadata)

      Ok(doc)
    }
  }
}

// Audit trail for LLM generations
type llmAuditEntry = {
  timestamp: float,
  promptType: llmPromptType,
  model: string,
  inputHash: string,
  outputHash: string,
  confidence: confidence,
  approved: bool,
  approver: option<string>,
}

let createAuditEntry = (
  response: llmResponse,
  promptType: llmPromptType,
  inputContext: string,
): llmAuditEntry => {
  {
    timestamp: Js.Date.now(),
    promptType: promptType,
    model: response.model,
    inputHash: Deduplicator.hashContent(inputContext),
    outputHash: Deduplicator.hashContent(response.content),
    confidence: response.confidence,
    approved: false,
    approver: None,
  }
}

// Export guardrails summary
let guardrailsSummary = (): string => {
  `
LLM Integration Guardrails
===========================

1. NEVER auto-commit LLM output (requiresApproval: always true)
2. Validate all LLM-generated content before use
3. Maintain audit trail of all LLM interactions
4. Never generate LICENSE files with LLM
5. Require human approval for all generated documentation
6. Tag all LLM-generated files with [LLM_GENERATED] prefix
7. Include confidence scores with all outputs
8. Validate output meets basic quality standards
9. Record model, timestamp, and context for all generations
10. Allow approval workflow before merging to main branch
`
}
